# # test_ollama.py
# from langchain_community.chat_models import ChatOllama

# llm = ChatOllama(model="mistral", temperature=0, streaming=False)
# resp = llm.invoke("¿Cuál es la capital de Francia?")
# print(resp.content)